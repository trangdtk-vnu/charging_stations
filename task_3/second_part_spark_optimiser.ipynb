{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, lit, udf, when\n",
    "from shapely.wkt import loads as load_wkt\n",
    "import osmnx as ox\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import joblib\n",
    "from sedona.register import SedonaRegistrator\n",
    "from sedona.core.SpatialRDD import PolygonRDD, PointRDD\n",
    "from sedona.utils.adapter import Adapter\n",
    "from sedona.core.SedonaContext import SedonaContext\n",
    "from sedona.core.enums import GridType\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9de769ce7ce5980",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Charging Stations in Italy\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.apache.sedona:sedona-python-adapter-3.4_2.12:1.3.1-incubating,org.datasyslab:geotools-wrapper:1.1.0-25.2\") \\\n",
    "    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n",
    "    .config(\"spark.kryo.registrator\", \"org.apache.sedona.viz.core.SedonaVizKryoRegistrator\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c13b1c240e9ebd",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sedona = SedonaContext.create(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4070bc22062be97c",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "spark.sparkContext.getConf().getAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a46da1bbd8e62df3",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Read the CSV file using Spark\n",
    "csv_path = r'C:\\Users\\devea\\Desktop\\dataset\\itdata\\ita_general_2020\\ita_general_2020.csv'\n",
    "df = spark.read.csv(csv_path, header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3237e682466767d3",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Fetch charging stations across Italy and save to GeoJSON\n",
    "charging_stations = ox.features_from_place('Italy', tags={'amenity': 'charging_station'})\n",
    "charging_stations.to_file(\" italy_charging_stations.geojson\", driver='GeoJSON')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6226da3275529abe",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sedona.core.SpatialRDD import PolygonRDD, PointRDD\n",
    "from sedona.utils.adapter import Adapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "808bc73618387af4",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "file_path = 'italy_charging_stations.geojson'\n",
    "charging_stations_gdf = gpd.read_file(file_path)\n",
    "\n",
    "duplicate_columns = charging_stations_gdf.columns[charging_stations_gdf.columns.duplicated()].tolist()\n",
    "print(\"Duplicate Columns:\", duplicate_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc3f4bfbaceda14",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "charging_stations_gdf = charging_stations_gdf.loc[:, ~charging_stations_gdf.columns.duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad23018026f486c",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "columns_to_drop = ['survey_date', 'socket:type3a', 'socket_type3a_output']\n",
    "    \n",
    "columns_to_drop = [col for col in columns_to_drop if col in charging_stations_gdf.columns]\n",
    "charging_stations_gdf = charging_stations_gdf.drop(columns=columns_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0636e9e8bffc9e",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "charging_stations_gdf.columns = [col.replace(':', '_') for col in charging_stations_gdf.columns]\n",
    "\n",
    "cleaned_file_path = 'italy_charging_stations_cleaned.geojson'\n",
    "charging_stations_gdf.to_file(cleaned_file_path, driver='GeoJSON')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b1996624aa16dc",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "file_path = 'italy_charging_stations_cleaned.geojson'\n",
    "gdf = gpd.read_file(file_path)\n",
    "\n",
    "# Print the CRS of the GeoDataFrame\n",
    "print(\"CRS of the GeoDataFrame:\", gdf.crs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e749f2e165a867f",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cs_df = spark.read.format(\"geojson\").load(\"italy_charging_stations_cleaned.geojson\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26866c479b580962",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "source_crs = \"EPSG:4326\"  \n",
    "target_crs = \"EPSG:4326\"\n",
    "css_df = Adapter.toSpatialRdd(cs_df, \"geometry\")\n",
    "css_df.CRSTransform(source_crs, target_crs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ced30534438528",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sedona.register import SedonaRegistrator\n",
    "from sedona.utils.adapter import Adapter\n",
    "from sedona.core.SpatialRDD import PolygonRDD\n",
    "from shapely.wkt import loads as load_wkt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a09c5e4577ed9a28",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load the grid CSV file into a Spark DataFrame\n",
    "grid_file_path = \"polygon_data.csv\"\n",
    "grid_df = spark.read.option(\"header\", \"true\").csv(grid_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d225a600d3e56b74",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "polygon_wkt_rdd = grid_df.filter(grid_df.polygon.isNotNull()).select(\"polygon\").rdd.map(lambda row: row['polygon'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe275a1855c7fee",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    # Create PolygonRDD from the RDD of WKT strings\n",
    "    polygon_rdd = PolygonRDD(\n",
    "        polygon_wkt_rdd,\n",
    "        \"epsg:4326\",  # Coordinate Reference System\n",
    "        True          # Carry spatial metadata\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while creating PolygonRDD: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acfdce382e3a4ce6",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "polygon_rdd.CRSTransform(source_crs, target_crs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ea4da8b0b34278",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Perform spatial partitioning (to improve join performance)\n",
    "polygon_rdd.spatialPartitioning(polygon_rdd.getPartitioner())\n",
    "css_df.spatialPartitioning(polygon_rdd.getPartitioner())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6efdc870fc721261",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Perform the spatial join between the grid polygons and the charging stations\n",
    "joined_rdd = polygon_rdd.spatialPartitionedJoin(css_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a70610f00ea3474",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "joined_df = Adapter.toDf(joined_rdd, spark)\n",
    "joined_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ddc5e668c0f0db",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af7bc0393c0bd4a",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "population_file_path = 'ita_general_2020.csv'\n",
    "population_df = spark.read.option(\"header\", \"true\").csv(population_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89362a4053424c24",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "population_df = population_df.withColumn(\"longitude\", population_df[\"longitude\"].cast(\"float\"))\n",
    "population_df = population_df.withColumn(\"latitude\", population_df[\"latitude\"].cast(\"float\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "231c8de780682715",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# using udf to check shapely\n",
    "@udf(\"boolean\")\n",
    "def point_in_polygon(longitude, latitude, polygon_wkt):\n",
    "    if longitude is None or latitude is None or polygon_wkt is None:\n",
    "        return False\n",
    "    try:\n",
    "        point = Point(longitude, latitude)\n",
    "        polygon = load_wkt(polygon_wkt)\n",
    "        return polygon.contains(point)\n",
    "    except:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1863f826bab68959",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "joined_df = population_df.crossJoin(grid_df) \\\n",
    "    .filter(point_in_polygon(col(\"longitude\"), col(\"latitude\"), col(\"polygon\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2033709d718d4d33",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "population_density_df = joined_df.groupBy(\"grid_id\").agg({\"ita_general_2020\": \"mean\"}).withColumnRenamed(\"avg(ita_general_2020)\", \"population_density\")\n",
    "population_density_df = population_density_df.fillna({\"population_density\": 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "452b6aba2c07d07e",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "result_df = grid_df.join(population_density_df, on=\"grid_id\", how=\"left\").fillna({\"population_density\": 0})\n",
    "result_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa4e8a0964c6fc15",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "features = {\n",
    "    'amenity': ['school', 'university', 'restaurant', 'place_of_worship', \n",
    "                'community_centre', 'townhall', 'parking', 'library'],\n",
    "    'leisure': ['park', 'cinema'],\n",
    "    'building': ['commercial', 'government', 'civic', 'retail']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d015f566dd1b623",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Extract POIs\n",
    "all_pois = []\n",
    "\n",
    "for feature_type, values in features.items():\n",
    "    for value in values:\n",
    "        tags = {feature_type: value}\n",
    "        try:\n",
    "            pois = ox.geometries_from_place('Italy', tags=tags)\n",
    "            pois = pois[['geometry']]\n",
    "            pois['poi_type'] = value\n",
    "            all_pois.append(pois)\n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting {value}: {e}\")\n",
    "\n",
    "# Combine POIs into a single GeoDataFrame\n",
    "pois_gdf = pd.concat(all_pois)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e5525b24522f50",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Landuse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c034afe4585b5d",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "categories = {\n",
    "    'residential': ['residential'],\n",
    "    'commercial': ['commercial', 'retail', 'office', 'industrial'],\n",
    "    'retail': ['retail'],\n",
    "    'industrial': ['industrial'],\n",
    "    'other': ['agricultural', 'forest', 'conservation', 'recreation']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "826fede1f1b4924b",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def categorize_landuse(polygon_wkt):\n",
    "    if polygon_wkt is None:\n",
    "        return 'Other'\n",
    "    try:\n",
    "        polygon = load_wkt(polygon_wkt)\n",
    "        landuse_gdf = ox.geometries_from_polygon(polygon, tags={'landuse': True})\n",
    "        if landuse_gdf.empty:\n",
    "            return 'Other'\n",
    "        counts = landuse_gdf['landuse'].value_counts()\n",
    "        for cat, types in categories.items():\n",
    "            if any(landuse in counts for landuse in types):\n",
    "                return cat\n",
    "    except Exception as e:\n",
    "        print(f\"Error categorizing land use: {str(e)}\")\n",
    "    return 'Other'\n",
    "\n",
    "categorize_landuse_udf = udf(categorize_landuse, \"string\")\n",
    "grid_df = grid_df.withColumn(\"landuse\", categorize_landuse_udf(col(\"polygon\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1566864838423025",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def aggregate_road_data(polygon_wkt):\n",
    "    if polygon_wkt is None:\n",
    "        return None, None, None, None\n",
    "    try:\n",
    "        polygon = load_wkt(polygon_wkt)\n",
    "        tags = {'highway': True}\n",
    "        roads = ox.geometries_from_polygon(polygon, tags)\n",
    "        if roads.empty:\n",
    "            return 'No roads', None, None, None\n",
    "        oneway = roads['oneway'].mode().iloc[0] if 'oneway' in roads else None\n",
    "        lanes = roads['lanes'].mode().iloc[0] if 'lanes' in roads else None\n",
    "        highway = roads['highway'].mode().iloc[0] if 'highway' in roads else None\n",
    "        if 'maxspeed' in roads:\n",
    "            roads['maxspeed'] = pd.to_numeric(roads['maxspeed'].str.replace(' km/h', '', regex=False), errors='coerce')\n",
    "            maxspeed = roads['maxspeed'].mean()\n",
    "        else:\n",
    "            maxspeed = None\n",
    "        return oneway, lanes, highway, maxspeed\n",
    "    except Exception as e:\n",
    "        print(f\"Error aggregating road data: {str(e)}\")\n",
    "        return None, None, None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b05926de88ef174f",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "aggregate_road_data_udf = udf(aggregate_road_data, \"struct<oneway:string, lanes:string, highway:string, maxspeed:double>\")\n",
    "grid_df = grid_df.withColumn(\"road_data\", aggregate_road_data_udf(col(\"polygon\")))\n",
    "grid_df = grid_df.withColumn(\"oneway_exists\", col(\"road_data.oneway\")) \\\n",
    "                 .withColumn(\"average_lanes\", col(\"road_data.lanes\")) \\\n",
    "                 .withColumn(\"high_way\", col(\"road_data.highway\")) \\\n",
    "                 .withColumn(\"average_maxspeed\", col(\"road_data.maxspeed\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "48dbb0cc7ab4de7d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-03T16:25:52.309221Z",
     "start_time": "2024-12-03T16:25:52.282464Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#  Network Data \n",
    "def extract_network_data(polygon_wkt):\n",
    "    if polygon_wkt is None:\n",
    "        return 0, 0, 0\n",
    "    try:\n",
    "        polygon = load_wkt(polygon_wkt)\n",
    "        G = ox.graph_from_polygon(polygon, network_type='drive', simplify=True)\n",
    "        nodes, edges = ox.graph_to_gdfs(G)\n",
    "        return len(nodes), len(edges), nx.density(G)\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting network data: {str(e)}\")\n",
    "        return 0, 0, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14746999523773d2",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "extract_network_data_udf = udf(extract_network_data, \"struct<nodes:int, edges:int, density:double>\")\n",
    "grid_df = grid_df.withColumn(\"network_data\", extract_network_data_udf(col(\"polygon\")))\n",
    "grid_df = grid_df.withColumn(\"nodes\", col(\"network_data.nodes\")) \\\n",
    "                 .withColumn(\"edges\", col(\"network_data.edges\")) \\\n",
    "                 .withColumn(\"density\", col(\"network_data.density\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "df0942bb2d48afca",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-03T16:26:24.150698Z",
     "start_time": "2024-12-03T16:26:24.147530Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Missing value\n",
    "grid_df = grid_df.fillna({\n",
    "    'landuse': 'Other',\n",
    "    'oneway_exists': 'no',\n",
    "    'average_lanes': grid_df.agg({\"average_lanes\": \"mode\"}).collect()[0][0] or 2.0,\n",
    "    'high_way': 'No data',\n",
    "    'average_maxspeed': grid_df.agg({\"average_maxspeed\": \"median\"}).collect()[0][0] or 50.0,\n",
    "    'nodes': 0,\n",
    "    'edges': 0,\n",
    "    'density': 0.0\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46716325e2191a69",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "grid_df = grid_df.drop(\"cinema_count\")\n",
    "\n",
    "# Save the Preprocessed DataFrame\n",
    "final_data_path = 'final_data.csv'\n",
    "grid_df.write.csv(final_data_path, header=True)\n",
    "\n",
    "# Step 9: Modeling Task\n",
    "# Load preprocessed data and trained model to make predictions\n",
    "input_data = pd.read_csv('EV_stat_YES.csv')\n",
    "if len(input_data.shape) == 1:\n",
    "    input_data = input_data.values.reshape(1, -1)\n",
    "\n",
    "# Load the model pipeline\n",
    "loaded_pipeline = joblib.load('svm_model.pkl')\n",
    "\n",
    "# Make predictions\n",
    "predictions = loaded_pipeline.predict(input_data)\n",
    "print(\"Predictions:\", predictions)\n",
    "\n",
    "# Step 10: Save Predictions\n",
    "input_data['output'] = predictions\n",
    "output_file = 'final_svm.csv'\n",
    "input_data.to_csv(output_file, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
